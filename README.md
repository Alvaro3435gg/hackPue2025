# README â€” Xenova/Qwen1.5-0.5B-Chat (ONNX)

## Modelo

Este proyecto utiliza el modelo **[Xenova/Qwen1.5-0.5B-Chat](https://huggingface.co/Xenova/Qwen1.5-0.5B-Chat)**, convertido a **ONNX** para su uso en navegador o Node.js mediante **Transformers.js**.

> ðŸ”¹ TambiÃ©n existe el repo base (no-chat): [Xenova/Qwen1.5-0.5B](https://huggingface.co/Xenova/Qwen1.5-0.5B).

Ambos estÃ¡n optimizados para ejecuciÃ³n local y soportan cuantizaciÃ³n.

---

## Estructura de archivos

Los archivos del modelo deben organizarse de la siguiente forma dentro del proyecto:

```
public/
â””â”€â”€ models/
    â””â”€â”€ Xenova/
        â””â”€â”€ Qwen1.5-0.5B-Chat/
            â””â”€â”€ onnx/
                â”œâ”€â”€ decoder_model_merged_quantized.onnx
                â”œâ”€â”€ config.json
                â”œâ”€â”€ generation_config.json
                â”œâ”€â”€ tokenizer_config.json
                â””â”€â”€ tokenizer.json
```

---

## InstalaciÃ³n

Instala la librerÃ­a oficial:

```bash
npm install @xenova/transformers
```

---

##  Uso rÃ¡pido

### 1. Ejecutar localmente en modo desarrollo

Si tu proyecto ya usa **Vite**, **Next.js** o similar, puedes correrlo con:

```bash
npm run dev
```

Esto levantarÃ¡ tu entorno local y podrÃ¡s acceder a la app que cargue el modelo desde `public/models`.

---

### 2. Hacer build para producciÃ³n

Cuando quieras generar la versiÃ³n optimizada:

```bash
npm run build
npx serve dist
```



